# -*- coding: utf-8 -*-
"""Assignment_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aiW2eWfJ9SWkHGHtF7QVJfuBz8fkAqkY
"""

#Import all the required libraries
from __future__ import print_function
from nltk.corpus.reader.knbc import test
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
import random
import nltk
import numpy
nltk.download('stopwords') #used when running for the first time

import sys

#Combine pos.txt and neg.txt that are saved to my google drive
filenames = [sys.argv[1]+'/pos.txt', sys.argv[1]+'/neg.txt']
with open(sys.argv[1]+'/negpos.txt', 'w') as outfile:
    for fname in filenames:
        with open(fname) as infile:
            for line in infile:
                outfile.write(line)



#Remove all special characters while tokenizing
tokenizer = RegexpTokenizer(r'\w+')
#Set the list of stopwords that will be used
stopWords = set(stopwords.words('english'))

#Counters to confirm distribution of tokens
trainingcounter = 0
validationcounter = 0
testcounter = 0

#Open the combined negpos.txt file to read and write to each file as follows:
#'out.csv' will be the tokenized data without special characters but with stopwords
#'out_ns.csv' will be the tokenized data without special characters and without stopwords
# 'train.csv', 'val.csv', 'test.csv' will have the 80%, 10%, 10% distribution but include stopwords
# 'train_ns.csv', 'val_ns.csv', 'test_ns.csv' will have the 80%, 10%, 10% distribution but exclude stopwords
with open (sys.argv[1]+'/negpos.txt') as fin,\
open(sys.argv[1]+'/out.csv','w') as fout1,\
open(sys.argv[1]+'/out_ns.csv','w') as fout2,\
open(sys.argv[1]+'/train_ns.csv','w') as fout3,\
open(sys.argv[1]+'/val_ns.csv','w') as fout4,\
open(sys.argv[1]+'/test_ns.csv','w') as fout5,\
open(sys.argv[1]+'/train.csv','w') as fout6,\
open(sys.argv[1]+'/val.csv','w') as fout7,\
open(sys.argv[1]+'/test.csv','w') as fout8:
    for line in fin: #read line by line
      wordsFiltered = [] #Re-initialize list to clear previous entries
      tokens = tokenizer.tokenize(line) #tokenize each line
      print(tokens, end='\n', file=fout1) #print to out.csv
      for w in tokens: #Check if tokens are in stopword list and append to wordsFiltered list
        if w not in stopWords:
          wordsFiltered.append(w)
      print(wordsFiltered, end='\n', file=fout2) #print filtered tokens to out_ns.csv
      x = numpy.random.uniform(low = 0.0, high = 1.0, size = None) #ranndom number generated with uniform distribution 
      if x < 0.8: #80% will be added here
        print(tokens, end='\n', file=fout6) #print to train.csv
        print(wordsFiltered, end='\n', file=fout3) #print to trai_ns.csv
        trainingcounter = trainingcounter + 1
      elif x >= 0.8 and x < 0.9: #10% will be added here
        print(tokens, end='\n', file=fout7) #print to val.csv
        print(wordsFiltered, end='\n', file=fout4) #print to val_ns.csv
        validationcounter = validationcounter + 1
      else: #remaining 10% will be added here
        print(tokens, end='\n', file=fout8) #print to test.csv
        print(wordsFiltered, end='\n', file=fout5) #print to test_ns.csv
        testcounter = testcounter + 1

    #Number confirmation
    
    total = trainingcounter + validationcounter + testcounter

    print('Training Counter: ')
    print(trainingcounter)
    print('Validation Counter: ')
    print(validationcounter)
    print('Test Coutner: ')
    print(testcounter)
    print('Total: ')
    print(total)

    print('Percentage of Training: ')
    print(round((trainingcounter/total)*100, 2))
    print('Percentage of Validation: ')
    print(round((validationcounter/total)*100, 2))
    print('Percentage of Test: ')
    print(round((testcounter/total)*100, 2))

